{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to ModelChimp\n\n\n\n  \n\n\n\n\nMachine learning practitioners should focus on experiments and not on managing them.\n\n\nInstallation\n\n\nInstall ModelChimp by executing the following command in the terminal\n\n\npip install -U modelchimp\n\n\n\nDefine the project\n\n\nName your project and give a description at the home page after registration.\n\n\n  \n\n\n\n\nCopy the project key from project page.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-modelchimp",
            "text": "Machine learning practitioners should focus on experiments and not on managing them.",
            "title": "Welcome to ModelChimp"
        },
        {
            "location": "/#installation",
            "text": "Install ModelChimp by executing the following command in the terminal  pip install -U modelchimp",
            "title": "Installation"
        },
        {
            "location": "/#define-the-project",
            "text": "Name your project and give a description at the home page after registration. \n     Copy the project key from project page.",
            "title": "Define the project"
        },
        {
            "location": "/track/",
            "text": "Tracker Class\n\n\nTracker class is instantiated with a project key and the domain name of Marble's ModelChimp server.\n\n\nfrom modelchimp import Tracker\n\n# Add before the trained model\ntracker = Tracker(key='<PROJECT KEY>',\n                  host='<host>',\n                  experiment_name=None,\n                  tracking=True,  \n                  auto_log=False,\n                  existing_exp_id=None)\n\n\n\nAttribute\n\n\n\n\nkey\n: Project key which is given in the ModelChimp project page\n\n\nhost\n: Domain name of the ModelChimp server\n\n\nexperiment_name(optional)\n: Give a custom name to the experiment. If not given then the experiment id is assigned\n\n\ntracking\n: if set to False then experiment won't be tracked\n\n\nauto_log\n: if set to True then ModelChimp automatically picks up the parameters of the model for which fit is called first. Currently, it works for sklearn only\n\n\nexisting_exp_id\n: Logs the details to an existing experiment for which the id is given\n\n\n\n\nMethods\n\n\nadd_custom_object()\n\n\nUpload a python object to ModelChimp\n\n\nadd_custom_object(name, object)\n\n\n\nArguments\n\n\n\n\nname:\n String Name of the object\n\n\nobject:\n Object Python object to be stored\n\n\n\n\nadd_dataset_id()\n\n\nAdd a dataset id for the experiment\n\n\nadd_dataset_id(id)\n\n\n\nArguments\n\n\n\n\nid:\n String|Int Id of the dataset\n\n\n\n\nadd_metric()\n\n\nAdd the metrics that need to be tracked\n\n\nadd_metric(metric_name, metric_value, epoch=None)\n\n\n\nArguments\n\n\n\n\nmetric_name:\n String Defines name of the metric\n\n\nmetric_value:\n Int|Float Represents the value of the metric\n\n\nepoch:\n Int If None then only 1 instance of the metric will be tracked\n\n\n\n\nadd_model_params()\n\n\nExtract the parameters out of a model object. Currently, only for sklearn objects.\n\n\nadd_model_params(model, model_name=None)\n\n\n\nArguments\n\n\n\n\nmodel:\n Model object whose parameters needs to be extracted\n\n\nmodel_name(optional):\n Name of model for which the model parameters wil be stored. The name will be prefixed to each of the model parameter.\n\n\n\n\nadd_multiple_metrics()\n\n\nAdd a dictionary of metrics\n\n\nadd_multiple_metrics(metrics_dict, epoch=None)\n\n\n\nArguments\n\n\n\n\nmetrics_dict:\n Dict Dictionary contains metrics to be tracked\n\n\nepoch:\n Int  If None then only 1 instance of each metric will be tracked\n\n\n\n\nadd_multiple_params()\n\n\nAdd a dictionary of parameters\n\n\nadd_multiple_params(params_dict)\n\n\n\nArguments\n\n\n\n\nparams_dict:\n Dict Dictionary containing parameters to be tracked\n\n\n\n\nadd_param()\n\n\nAdd the parameters that need to be tracked\n\n\nadd_param(param_name, param_value)\n\n\n\nArguments\n\n\n\n\nparam_name:\n String which defines name of the parameter\n\n\nparam_value:\n Int|Float|Bool|String that represents the value of the parameter\n\n\n\n\npull_custom_object()\n\n\nPull python object stored in ModelChimp server\n\n\npull_custom_object(id)\n\n\n\nArguments\n\n\n\n\nid:\n String Id of the object given in the ModelChimp web portal\n\n\n\n\npull_params()\n\n\nPull parameters of an experiment locally\n\n\npull_params(experiment_id)\n\n\n\nArguments\n\n\n\n\nexperiment_id:\n String Experiment id of the experiment. For example: experiment_id='11f46fec228120cce93cea0bfa0c06b2a8e9a7ebc6e91f1ddb2c145901ea8259'",
            "title": "Python API"
        },
        {
            "location": "/track/#tracker-class",
            "text": "Tracker class is instantiated with a project key and the domain name of Marble's ModelChimp server.  from modelchimp import Tracker\n\n# Add before the trained model\ntracker = Tracker(key='<PROJECT KEY>',\n                  host='<host>',\n                  experiment_name=None,\n                  tracking=True,  \n                  auto_log=False,\n                  existing_exp_id=None)",
            "title": "Tracker Class"
        },
        {
            "location": "/track/#attribute",
            "text": "key : Project key which is given in the ModelChimp project page  host : Domain name of the ModelChimp server  experiment_name(optional) : Give a custom name to the experiment. If not given then the experiment id is assigned  tracking : if set to False then experiment won't be tracked  auto_log : if set to True then ModelChimp automatically picks up the parameters of the model for which fit is called first. Currently, it works for sklearn only  existing_exp_id : Logs the details to an existing experiment for which the id is given",
            "title": "Attribute"
        },
        {
            "location": "/track/#methods",
            "text": "",
            "title": "Methods"
        },
        {
            "location": "/track/#add_custom_object",
            "text": "Upload a python object to ModelChimp  add_custom_object(name, object)  Arguments   name:  String Name of the object  object:  Object Python object to be stored",
            "title": "add_custom_object()"
        },
        {
            "location": "/track/#add_dataset_id",
            "text": "Add a dataset id for the experiment  add_dataset_id(id)  Arguments   id:  String|Int Id of the dataset",
            "title": "add_dataset_id()"
        },
        {
            "location": "/track/#add_metric",
            "text": "Add the metrics that need to be tracked  add_metric(metric_name, metric_value, epoch=None)  Arguments   metric_name:  String Defines name of the metric  metric_value:  Int|Float Represents the value of the metric  epoch:  Int If None then only 1 instance of the metric will be tracked",
            "title": "add_metric()"
        },
        {
            "location": "/track/#add_model_params",
            "text": "Extract the parameters out of a model object. Currently, only for sklearn objects.  add_model_params(model, model_name=None)  Arguments   model:  Model object whose parameters needs to be extracted  model_name(optional):  Name of model for which the model parameters wil be stored. The name will be prefixed to each of the model parameter.",
            "title": "add_model_params()"
        },
        {
            "location": "/track/#add_multiple_metrics",
            "text": "Add a dictionary of metrics  add_multiple_metrics(metrics_dict, epoch=None)  Arguments   metrics_dict:  Dict Dictionary contains metrics to be tracked  epoch:  Int  If None then only 1 instance of each metric will be tracked",
            "title": "add_multiple_metrics()"
        },
        {
            "location": "/track/#add_multiple_params",
            "text": "Add a dictionary of parameters  add_multiple_params(params_dict)  Arguments   params_dict:  Dict Dictionary containing parameters to be tracked",
            "title": "add_multiple_params()"
        },
        {
            "location": "/track/#add_param",
            "text": "Add the parameters that need to be tracked  add_param(param_name, param_value)  Arguments   param_name:  String which defines name of the parameter  param_value:  Int|Float|Bool|String that represents the value of the parameter",
            "title": "add_param()"
        },
        {
            "location": "/track/#pull_custom_object",
            "text": "Pull python object stored in ModelChimp server  pull_custom_object(id)  Arguments   id:  String Id of the object given in the ModelChimp web portal",
            "title": "pull_custom_object()"
        },
        {
            "location": "/track/#pull_params",
            "text": "Pull parameters of an experiment locally  pull_params(experiment_id)  Arguments   experiment_id:  String Experiment id of the experiment. For example: experiment_id='11f46fec228120cce93cea0bfa0c06b2a8e9a7ebc6e91f1ddb2c145901ea8259'",
            "title": "pull_params()"
        },
        {
            "location": "/tensorflow/",
            "text": "Tensorflow Example\n\n\nhttps://github.com/ModelChimp/tensorflow_example\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport math\nimport timeit\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nFLAGS = None\n\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\ndef deepnn(x):\n  x_image = tf.reshape(x, [-1, 28, 28, 1])\n  W_conv1 = weight_variable([5, 5, 1, 32])\n  b_conv1 = bias_variable([32])\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  h_pool1 = max_pool_2x2(h_conv1)\n  W_conv2 = weight_variable([5, 5, 32, 64])\n  b_conv2 = bias_variable([64])\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n  h_pool2 = max_pool_2x2(h_conv2)\n  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n  b_fc1 = bias_variable([1024])\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n  keep_prob = tf.placeholder(tf.float32)\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n  W_fc2 = weight_variable([1024, 10])\n  b_fc2 = bias_variable([10])\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n  return y_conv, keep_prob\n\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n\ndef main(_):\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n  param = {}\n\n  param['data_length'] = mnist.train._num_examples\n  param['batch_size'] = 200\n  param['epoch_num'] = 10\n  param['steps_num'] = math.floor(param['data_length'] / param['batch_size']) - 1\n  param['optimzer'] = 'Adam'\n  param['cost_function'] = 'cross entropy'\n  param['learning_rate'] = 0.01\n\n  # Create the model\n  x = tf.placeholder(tf.float32, [None, 784])\n\n  # Define loss and optimizer\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  # Build the graph for the deep net\n  y_conv, keep_prob = deepnn(x)\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n  train_step = tf.train.AdamOptimizer(param['learning_rate']).minimize(cross_entropy)\n  correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  tracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification') #MODELCHIMP\n  tracker.add_multiple_params(param) #MODELCHIMP\n\n  with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for i in range(param['epoch_num']):\n        # snapshot start time\n        start_time = timeit.default_timer()\n\n        for j in range(param['steps_num']):\n          batch = mnist.train.next_batch(50)\n          train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n        # snapshot start time\n        end_time = timeit.default_timer()\n\n        train_accuracy = accuracy.eval(feed_dict={\n            x: batch[0], y_: batch[1], keep_prob: 1.0})\n\n        test_accuracy = accuracy.eval(feed_dict={\n            x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n        tracker.add_metric(\"accuracy_train\", float(train_accuracy), i) #MODELCHIMP\n        tracker.add_metric(\"accuracy_test\", float(test_accuracy), i) #MODELCHIMP\n        tracker.add_duration_at_epoch(\"Train\", end_time - start_time, i) #MODELCHIMP\n\n        print(\"Epoch %s: train_accuracy = %s, test_accuracy = %s\" % (i, train_accuracy, test_accuracy))\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str,\n                      default='/tmp/tensorflow/mnist/input_data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)",
            "title": "TensorFlow Example"
        },
        {
            "location": "/tensorflow/#tensorflow-example",
            "text": "https://github.com/ModelChimp/tensorflow_example  from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\nimport math\nimport timeit\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nFLAGS = None\n\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\ndef deepnn(x):\n  x_image = tf.reshape(x, [-1, 28, 28, 1])\n  W_conv1 = weight_variable([5, 5, 1, 32])\n  b_conv1 = bias_variable([32])\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  h_pool1 = max_pool_2x2(h_conv1)\n  W_conv2 = weight_variable([5, 5, 32, 64])\n  b_conv2 = bias_variable([64])\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n  h_pool2 = max_pool_2x2(h_conv2)\n  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n  b_fc1 = bias_variable([1024])\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n  keep_prob = tf.placeholder(tf.float32)\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n  W_fc2 = weight_variable([1024, 10])\n  b_fc2 = bias_variable([10])\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n  return y_conv, keep_prob\n\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n\ndef main(_):\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n  param = {}\n\n  param['data_length'] = mnist.train._num_examples\n  param['batch_size'] = 200\n  param['epoch_num'] = 10\n  param['steps_num'] = math.floor(param['data_length'] / param['batch_size']) - 1\n  param['optimzer'] = 'Adam'\n  param['cost_function'] = 'cross entropy'\n  param['learning_rate'] = 0.01\n\n  # Create the model\n  x = tf.placeholder(tf.float32, [None, 784])\n\n  # Define loss and optimizer\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  # Build the graph for the deep net\n  y_conv, keep_prob = deepnn(x)\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n  train_step = tf.train.AdamOptimizer(param['learning_rate']).minimize(cross_entropy)\n  correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n  tracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification') #MODELCHIMP\n  tracker.add_multiple_params(param) #MODELCHIMP\n\n  with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n\n      for i in range(param['epoch_num']):\n        # snapshot start time\n        start_time = timeit.default_timer()\n\n        for j in range(param['steps_num']):\n          batch = mnist.train.next_batch(50)\n          train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\n        # snapshot start time\n        end_time = timeit.default_timer()\n\n        train_accuracy = accuracy.eval(feed_dict={\n            x: batch[0], y_: batch[1], keep_prob: 1.0})\n\n        test_accuracy = accuracy.eval(feed_dict={\n            x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n        tracker.add_metric(\"accuracy_train\", float(train_accuracy), i) #MODELCHIMP\n        tracker.add_metric(\"accuracy_test\", float(test_accuracy), i) #MODELCHIMP\n        tracker.add_duration_at_epoch(\"Train\", end_time - start_time, i) #MODELCHIMP\n\n        print(\"Epoch %s: train_accuracy = %s, test_accuracy = %s\" % (i, train_accuracy, test_accuracy))\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str,\n                      default='/tmp/tensorflow/mnist/input_data',\n                      help='Directory for storing input data')\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)",
            "title": "Tensorflow Example"
        },
        {
            "location": "/keras/",
            "text": "Keras Example\n\n\nhttps://github.com/ModelChimp/keras_example\n\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\n# MODELCHIMP\nfrom modelchimp import Tracker\nfrom modelchimp.keras import ModelChimpCallback\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:2000]\ny_train = y_train[:2000]\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n\nparam = {\n    'loss' : keras.losses.categorical_crossentropy,\n    'optimizer' : keras.optimizers.Adadelta(),\n    'batch_size' : batch_size,\n    'epochs' : epochs\n}\n\n# MODELCHIMP Tracker instantiation\ntracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification')\ntracker.add_multiple_params(param)\n\nmodel.compile(loss=param['loss'],\n              optimizer=param['optimizer'],\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=param['batch_size'],\n          epochs=param['batch_size'],\n          verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[ModelChimpCallback(),]) # MODELCHIMP Callback",
            "title": "Keras Example"
        },
        {
            "location": "/keras/#keras-example",
            "text": "https://github.com/ModelChimp/keras_example  from __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\n# MODELCHIMP\nfrom modelchimp import Tracker\nfrom modelchimp.keras import ModelChimpCallback\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train[:2000]\ny_train = y_train[:2000]\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n\nparam = {\n    'loss' : keras.losses.categorical_crossentropy,\n    'optimizer' : keras.optimizers.Adadelta(),\n    'batch_size' : batch_size,\n    'epochs' : epochs\n}\n\n# MODELCHIMP Tracker instantiation\ntracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification')\ntracker.add_multiple_params(param)\n\nmodel.compile(loss=param['loss'],\n              optimizer=param['optimizer'],\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train,\n          batch_size=param['batch_size'],\n          epochs=param['batch_size'],\n          verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[ModelChimpCallback(),]) # MODELCHIMP Callback",
            "title": "Keras Example"
        },
        {
            "location": "/pytorch/",
            "text": "PyTorch Example\n\n\nhttps://github.com/ModelChimp/pytorch_example\n\n\nfrom __future__ import print_function\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    train_loss = 0\n    correct = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        train_loss += F.nll_loss(output, target, reduction='sum').item()\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\n    train_loss /= len(train_loader.dataset)\n    accuracy = correct / len(train_loader.dataset)\n\n    return {\n        'train_loss': train_loss,\n        'train_accuracy': accuracy\n    }\n\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = correct / len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    return {\n        'test_loss': test_loss,\n        'test_accuracy': accuracy\n    }\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                        help='SGD momentum (default: 0.5)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    param = args.__dict__\n\n    torch.manual_seed(args.seed)\n\n    # MODELCHIMP Tracker\n    tracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification')\n    tracker.add_multiple_params(param)\n\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    for epoch in range(1, args.epochs + 1):\n        train_metric = train(args, model, device, train_loader, optimizer, epoch)\n        test_metric = test(args, model, device, test_loader)\n\n        # MODELCHIMP Tracker\n        tracker.add_multiple_metrics(train_metric, epoch=epoch)\n        tracker.add_multiple_metrics(test_metric, epoch=epoch)\n\n\nif __name__ == '__main__':\n    main()",
            "title": "PyTorch Example"
        },
        {
            "location": "/pytorch/#pytorch-example",
            "text": "https://github.com/ModelChimp/pytorch_example  from __future__ import print_function\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    train_loss = 0\n    correct = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        train_loss += F.nll_loss(output, target, reduction='sum').item()\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\n    train_loss /= len(train_loader.dataset)\n    accuracy = correct / len(train_loader.dataset)\n\n    return {\n        'train_loss': train_loss,\n        'train_accuracy': accuracy\n    }\n\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = correct / len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n    return {\n        'test_loss': test_loss,\n        'test_accuracy': accuracy\n    }\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 10)')\n    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                        help='learning rate (default: 0.01)')\n    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                        help='SGD momentum (default: 0.5)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                        help='how many batches to wait before logging training status')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    param = args.__dict__\n\n    torch.manual_seed(args.seed)\n\n    # MODELCHIMP Tracker\n    tracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification')\n    tracker.add_multiple_params(param)\n\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    for epoch in range(1, args.epochs + 1):\n        train_metric = train(args, model, device, train_loader, optimizer, epoch)\n        test_metric = test(args, model, device, test_loader)\n\n        # MODELCHIMP Tracker\n        tracker.add_multiple_metrics(train_metric, epoch=epoch)\n        tracker.add_multiple_metrics(test_metric, epoch=epoch)\n\n\nif __name__ == '__main__':\n    main()",
            "title": "PyTorch Example"
        },
        {
            "location": "/pyspark/",
            "text": "PySpark Example\n\n\nhttps://github.com/ModelChimp/pyspark_example\n\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SQLContext\n\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\nsqlContext = SQLContext(sc)\n\n# Load training data\ntrain = sqlContext.read.format(\"csv\").option(\"header\", 'true').load(\"train.csv\")\n\ntrain = train.select(col(\"Survived\"),col(\"Sex\"),col(\"Embarked\"),col(\"Pclass\").cast(\"float\"),col(\"Age\").cast(\"float\"),col(\"SibSp\").cast(\"float\"),col(\"Fare\").cast(\"float\"))\n\n# dropping null values\ntrain = train.dropna()\n\n# Spliting in train and test set. Beware : It sorts the dataset\n(traindf, testdf) = train.randomSplit([0.7,0.3])\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\ngenderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\nembarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n\nsurviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n\n# One Hot Encoder on indexed features\ngenderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\nembarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n\n# Create the vector structured data (label,features(vector))\nassembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n\n# MODELCHIMP Tracking Code\ntracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification') #MODELCHIMP\n\n# Train a RandomForest model.\nparam = {\n    'numTrees' : 100,\n    'impurity' : \"entropy\"\n}\nrf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\", numTrees=param['numTrees'], impurity=param['impurity'])\ntracker.add_multiple_params(param) #MODELCHIMP\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[surviveIndexer, genderIndexer, embarkIndexer, genderEncoder,embarkEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(traindf)\n\n# Predictions\npredictions = model.transform(testdf)\n\n# Select example rows to display.\npredictions.columns\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"Survived\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\npredictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[6]\nprint(rfModel)  # summary only\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\ntracker.add_metric(\"Accuracy\", accuracy) #MODELCHIMP\n\nevaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluatorf1.evaluate(predictions)\nprint(\"f1 = %g\" % f1)\ntracker.add_metric(\"f1\", f1) #MODELCHIMP\n\nevaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nwp = evaluatorwp.evaluate(predictions)\nprint(\"weightedPrecision = %g\" % wp)\ntracker.add_metric(\"weightedPrecision\", wp) #MODELCHIMP\n\nevaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nwr = evaluatorwr.evaluate(predictions)\nprint(\"weightedRecall = %g\" % wr)\ntracker.add_metric(\"weightedRecall\", wr) #MODELCHIMP",
            "title": "PySpark Example"
        },
        {
            "location": "/pyspark/#pyspark-example",
            "text": "https://github.com/ModelChimp/pyspark_example  from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import SQLContext\n\n# MODELCHIMP tracker\nfrom modelchimp import Tracker\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\nsqlContext = SQLContext(sc)\n\n# Load training data\ntrain = sqlContext.read.format(\"csv\").option(\"header\", 'true').load(\"train.csv\")\n\ntrain = train.select(col(\"Survived\"),col(\"Sex\"),col(\"Embarked\"),col(\"Pclass\").cast(\"float\"),col(\"Age\").cast(\"float\"),col(\"SibSp\").cast(\"float\"),col(\"Fare\").cast(\"float\"))\n\n# dropping null values\ntrain = train.dropna()\n\n# Spliting in train and test set. Beware : It sorts the dataset\n(traindf, testdf) = train.randomSplit([0.7,0.3])\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\ngenderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\nembarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n\nsurviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n\n# One Hot Encoder on indexed features\ngenderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\nembarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n\n# Create the vector structured data (label,features(vector))\nassembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n\n# MODELCHIMP Tracking Code\ntracker = Tracker('<PROJECT KEY>', host='demo.modelchimp.com', experiment_name='MNIST Classification') #MODELCHIMP\n\n# Train a RandomForest model.\nparam = {\n    'numTrees' : 100,\n    'impurity' : \"entropy\"\n}\nrf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\", numTrees=param['numTrees'], impurity=param['impurity'])\ntracker.add_multiple_params(param) #MODELCHIMP\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[surviveIndexer, genderIndexer, embarkIndexer, genderEncoder,embarkEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(traindf)\n\n# Predictions\npredictions = model.transform(testdf)\n\n# Select example rows to display.\npredictions.columns\n\n# Select example rows to display.\npredictions.select(\"prediction\", \"Survived\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\npredictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\n\nrfModel = model.stages[6]\nprint(rfModel)  # summary only\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Accuracy = %g\" % accuracy)\ntracker.add_metric(\"Accuracy\", accuracy) #MODELCHIMP\n\nevaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\nf1 = evaluatorf1.evaluate(predictions)\nprint(\"f1 = %g\" % f1)\ntracker.add_metric(\"f1\", f1) #MODELCHIMP\n\nevaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\nwp = evaluatorwp.evaluate(predictions)\nprint(\"weightedPrecision = %g\" % wp)\ntracker.add_metric(\"weightedPrecision\", wp) #MODELCHIMP\n\nevaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\nwr = evaluatorwr.evaluate(predictions)\nprint(\"weightedRecall = %g\" % wr)\ntracker.add_metric(\"weightedRecall\", wr) #MODELCHIMP",
            "title": "PySpark Example"
        },
        {
            "location": "/scikit/",
            "text": "Scikit Example\n\n\nhttps://github.com/ModelChimp/scikit_example\n\n\nfrom modelchimp import Tracker\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Load and split the data\nbreast_cancer = load_breast_cancer()\nx_train, x_test, y_train, y_test = train_test_split(\n    breast_cancer.data,\n    breast_cancer.target,\n    stratify=breast_cancer.target,\n    random_state=49)\n\n# Train and predict\n\nfor n in [10,30,80,100,]:\n    tracker = Tracker('<PROJECT KEY>', 'demo.modelchimp.com')\n\n    random_forest_model = RandomForestClassifier(n_estimators=101).fit(x_train, y_train)\n    y_pred = random_forest_model.predict(x_test)\n\n    # Get the evaluation metric\n    evl = {}\n    evl['accuracy'] = accuracy_score(y_test, y_pred)\n    evl['precision'] = precision_score(y_test, y_pred)\n    evl['recall'] = recall_score(y_test, y_pred)\n\n    # MODELCHIMP: Log the metric\n    tracker.add_multiple_metrics(evl)\n\n    print(\"Accuracy: %s\" % evl['accuracy'])\n    print(\"Precision: %s\" % evl['precision'])\n    print(\"Recall: %s\" % evl['recall'])",
            "title": "SciKit Example"
        },
        {
            "location": "/scikit/#scikit-example",
            "text": "https://github.com/ModelChimp/scikit_example  from modelchimp import Tracker\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Load and split the data\nbreast_cancer = load_breast_cancer()\nx_train, x_test, y_train, y_test = train_test_split(\n    breast_cancer.data,\n    breast_cancer.target,\n    stratify=breast_cancer.target,\n    random_state=49)\n\n# Train and predict\n\nfor n in [10,30,80,100,]:\n    tracker = Tracker('<PROJECT KEY>', 'demo.modelchimp.com')\n\n    random_forest_model = RandomForestClassifier(n_estimators=101).fit(x_train, y_train)\n    y_pred = random_forest_model.predict(x_test)\n\n    # Get the evaluation metric\n    evl = {}\n    evl['accuracy'] = accuracy_score(y_test, y_pred)\n    evl['precision'] = precision_score(y_test, y_pred)\n    evl['recall'] = recall_score(y_test, y_pred)\n\n    # MODELCHIMP: Log the metric\n    tracker.add_multiple_metrics(evl)\n\n    print(\"Accuracy: %s\" % evl['accuracy'])\n    print(\"Precision: %s\" % evl['precision'])\n    print(\"Recall: %s\" % evl['recall'])",
            "title": "Scikit Example"
        }
    ]
}